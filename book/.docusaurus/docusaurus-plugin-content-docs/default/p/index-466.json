{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/","label":"Physical AI & Humanoid Robotics","docId":"index","unlisted":false},{"type":"category","label":"Module 1: Physical AI Foundations","items":[{"type":"link","href":"/foundations/intro","label":"Module 1: Physical AI Foundations","docId":"foundations/intro","unlisted":false},{"type":"link","href":"/foundations/embodied-intelligence","label":"Embodied Intelligence","docId":"foundations/embodied-intelligence","unlisted":false},{"type":"link","href":"/foundations/ros2-intro","label":"ROS 2 Introduction","docId":"foundations/ros2-intro","unlisted":false},{"type":"link","href":"/foundations/exercises","label":"Module 1: Exercises & Solutions","docId":"foundations/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: Digital Twins & Gazebo","items":[{"type":"link","href":"/simulation/intro","label":"Module 2: Digital Twins & Gazebo Simulation","docId":"simulation/intro","unlisted":false},{"type":"link","href":"/simulation/gazebo-fundamentals","label":"Gazebo Fundamentals","docId":"simulation/gazebo-fundamentals","unlisted":false},{"type":"link","href":"/simulation/setup-gazebo","label":"Setting Up Gazebo for Humanoid Robotics","docId":"simulation/setup-gazebo","unlisted":false},{"type":"link","href":"/simulation/urdf-humanoid","label":"URDF: Describing Humanoid Robots","docId":"simulation/urdf-humanoid","unlisted":false},{"type":"link","href":"/simulation/exercises","label":"Module 2: Exercises & Solutions","docId":"simulation/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: Perception & NVIDIA Isaac","items":[{"type":"link","href":"/perception/intro","label":"Module 3: Perception & Sim-to-Real Transfer","docId":"perception/intro","unlisted":false},{"type":"link","href":"/perception/sensor-fusion","label":"Sensor Fusion & Localization","docId":"perception/sensor-fusion","unlisted":false},{"type":"link","href":"/perception/sim-to-real-transfer","label":"Sim-to-Real Transfer: Bridging Simulation and Reality","docId":"perception/sim-to-real-transfer","unlisted":false},{"type":"link","href":"/perception/isaac-workflows","label":"NVIDIA Isaac: Photorealistic Simulation & Synthetic Data Generation","docId":"perception/isaac-workflows","unlisted":false},{"type":"link","href":"/perception/setup-isaac","label":"Setting Up NVIDIA Isaac Sim","docId":"perception/setup-isaac","unlisted":false},{"type":"category","label":"Diagrams","items":[{"type":"link","href":"/perception/diagrams/domain-randomization","label":"Domain Randomization Visualization","docId":"perception/diagrams/domain-randomization","unlisted":false},{"type":"link","href":"/perception/diagrams/sim-to-real-gaps","label":"Sim-to-Real Transfer Gaps Visualization","docId":"perception/diagrams/sim-to-real-gaps","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"link","href":"/perception/exercises","label":"Module 3: Perception & Sim-to-Real Transfer Exercises","docId":"perception/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action Systems","items":[{"type":"link","href":"/vla-systems/intro","label":"Module 4: Vision-Language-Action Systems & Embodied AI","docId":"vla-systems/intro","unlisted":false},{"type":"link","href":"/vla-systems/vision-language-models","label":"Vision-Language Models for Robotics","docId":"vla-systems/vision-language-models","unlisted":false},{"type":"link","href":"/vla-systems/vla-architecture","label":"VLA System Architecture","docId":"vla-systems/vla-architecture","unlisted":false},{"type":"link","href":"/vla-systems/llm-planning","label":"LLM-Based Task Planning for Robotics","docId":"vla-systems/llm-planning","unlisted":false},{"type":"link","href":"/vla-systems/prompting-strategies","label":"Prompting Strategies for Robotics","docId":"vla-systems/prompting-strategies","unlisted":false},{"type":"link","href":"/vla-systems/action-grounding","label":"Action Grounding in Robotics","docId":"vla-systems/action-grounding","unlisted":false},{"type":"link","href":"/vla-systems/embodied-reasoning","label":"Embodied Reasoning in Robotics","docId":"vla-systems/embodied-reasoning","unlisted":false},{"type":"link","href":"/vla-systems/end-to-end-pipeline","label":"Vision-Language-Action Systems: End-to-End Pipeline","docId":"vla-systems/end-to-end-pipeline","unlisted":false},{"type":"link","href":"/vla-systems/voice-to-action","label":"End-to-End Voice-to-Action Pipeline","docId":"vla-systems/voice-to-action","unlisted":false},{"type":"link","href":"/vla-systems/lora-adaptation","label":"Fine-Tuning LLMs with LoRA Adaptation","docId":"vla-systems/lora-adaptation","unlisted":false},{"type":"link","href":"/vla-systems/setup-llm","label":"Setup Guide: Large Language Models for Robotics","docId":"vla-systems/setup-llm","unlisted":false},{"type":"link","href":"/vla-systems/exercises","label":"Module 4 Exercises: Vision-Language-Action Systems","docId":"vla-systems/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 5: End-to-End Learning & Diffusion Models","items":[{"type":"link","href":"/embodied-learning/intro","label":"Module 5: End-to-End Learning & Diffusion Models","docId":"embodied-learning/intro","unlisted":false},{"type":"link","href":"/embodied-learning/learning-spectrum","label":"The Learning Spectrum: BC vs Diffusion vs RL","docId":"embodied-learning/learning-spectrum","unlisted":false},{"type":"link","href":"/embodied-learning/imitation-learning","label":"Imitation Learning: Learning from Demonstrations","docId":"embodied-learning/imitation-learning","unlisted":false},{"type":"link","href":"/embodied-learning/reinforcement-learning","label":"Reinforcement Learning for Embodied AI","docId":"embodied-learning/reinforcement-learning","unlisted":false},{"type":"link","href":"/embodied-learning/diffusion-for-robotics","label":"Diffusion Models for Robot Control","docId":"embodied-learning/diffusion-for-robotics","unlisted":false},{"type":"link","href":"/embodied-learning/end-to-end-learning","label":"End-to-End Learning: Architectures & Strategies","docId":"embodied-learning/end-to-end-learning","unlisted":false},{"type":"link","href":"/embodied-learning/training-pipeline","label":"Complete Training Pipeline: From Data to Deployment","docId":"embodied-learning/training-pipeline","unlisted":false},{"type":"link","href":"/embodied-learning/exercises","label":"Module 5: End-to-End Learning - Exercises","docId":"embodied-learning/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 6: Scaling Systems & Production","items":[{"type":"link","href":"/scaling-systems/intro","label":"Module 6: Scaling & Production Systems","docId":"scaling-systems/intro","unlisted":false},{"type":"link","href":"/scaling-systems/scaling-pipeline","label":"Scaling Pipeline: End-to-End Production Workflow","docId":"scaling-systems/scaling-pipeline","unlisted":false},{"type":"link","href":"/scaling-systems/distributed-training","label":"Distributed Training: Multi-GPU & Multi-Robot Systems","docId":"scaling-systems/distributed-training","unlisted":false},{"type":"link","href":"/scaling-systems/multi-task-learning","label":"Multi-Task Learning: Training at Scale","docId":"scaling-systems/multi-task-learning","unlisted":false},{"type":"link","href":"/scaling-systems/benchmarking-framework","label":"Benchmarking at Scale: Evaluation Framework","docId":"scaling-systems/benchmarking-framework","unlisted":false},{"type":"link","href":"/scaling-systems/cost-analysis","label":"Cost Analysis: Economics of Robotic Automation","docId":"scaling-systems/cost-analysis","unlisted":false},{"type":"link","href":"/scaling-systems/fleet-architecture","label":"Fleet Architecture: System Design at Scale","docId":"scaling-systems/fleet-architecture","unlisted":false},{"type":"link","href":"/scaling-systems/real-robot-deployment","label":"Real Robot Deployment: From Simulation to Production","docId":"scaling-systems/real-robot-deployment","unlisted":false},{"type":"link","href":"/scaling-systems/exercises","label":"Exercises: Production Robotics & Scaling Systems","docId":"scaling-systems/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 7: Capstone & Real-World Deployment","items":[{"type":"link","href":"/capstone-deployment/intro","label":"Module 7: Capstone & Real-World Deployment","docId":"capstone-deployment/intro","unlisted":false},{"type":"link","href":"/capstone-deployment/production-architecture","label":"Section 1: Production System Architecture","docId":"capstone-deployment/production-architecture","unlisted":false},{"type":"link","href":"/capstone-deployment/deployment-strategies","label":"Section 2: Deployment Strategies","docId":"capstone-deployment/deployment-strategies","unlisted":false},{"type":"link","href":"/capstone-deployment/operations-maintenance","label":"Section 3: Operations & Maintenance","docId":"capstone-deployment/operations-maintenance","unlisted":false},{"type":"link","href":"/capstone-deployment/case-studies","label":"Section 4: Real-World Case Studies","docId":"capstone-deployment/case-studies","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Capstone Project","items":[{"type":"link","href":"/capstone/architecture","label":"Capstone Project: System Architecture","docId":"capstone/architecture","unlisted":false},{"type":"link","href":"/capstone/setup","label":"Capstone Project: Setup Guide","docId":"capstone/setup","unlisted":false},{"type":"link","href":"/capstone/running-the-system","label":"Capstone Project: Running the System","docId":"capstone/running-the-system","unlisted":false},{"type":"link","href":"/capstone/extensions","label":"Capstone Project: Extensions","docId":"capstone/extensions","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Appendices","items":[{"type":"link","href":"/glossary","label":"Glossary","docId":"glossary","unlisted":false},{"type":"link","href":"/references","label":"References & Further Reading","docId":"references","unlisted":false},{"type":"link","href":"/troubleshooting","label":"Troubleshooting","docId":"troubleshooting","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"capstone-deployment/case-studies":{"id":"capstone-deployment/case-studies","title":"Section 4: Real-World Case Studies","description":"Learning from 500K+ robots in production.","sidebar":"tutorialSidebar"},"capstone-deployment/deployment-strategies":{"id":"capstone-deployment/deployment-strategies","title":"Section 2: Deployment Strategies","description":"Safely deploying models to 100+ robots with zero downtime.","sidebar":"tutorialSidebar"},"capstone-deployment/intro":{"id":"capstone-deployment/intro","title":"Module 7: Capstone & Real-World Deployment","description":"From research lab to production robots in real factories.","sidebar":"tutorialSidebar"},"capstone-deployment/operations-maintenance":{"id":"capstone-deployment/operations-maintenance","title":"Section 3: Operations & Maintenance","description":"Running 100+ robots 24/7 without breaking production.","sidebar":"tutorialSidebar"},"capstone-deployment/production-architecture":{"id":"capstone-deployment/production-architecture","title":"Section 1: Production System Architecture","description":"Complete system design for deploying 100+ robots in real factories.","sidebar":"tutorialSidebar"},"capstone/architecture":{"id":"capstone/architecture","title":"Capstone Project: System Architecture","description":"This document outlines the architecture for the full, end-to-end autonomous humanoid system. The goal of this capstone is to integrate all the concepts from the previous modules into a single, runnable project that can accept spoken commands and perform actions in a simulated environment.","sidebar":"tutorialSidebar"},"capstone/extensions":{"id":"capstone/extensions","title":"Capstone Project: Extensions","description":"The capstone project is designed to be a foundation that you can build upon. This guide provides ideas and code templates for extending the system's capabilities.","sidebar":"tutorialSidebar"},"capstone/running-the-system":{"id":"capstone/running-the-system","title":"Capstone Project: Running the System","description":"This guide explains how to launch and interact with the complete humanoid robot capstone system in the Gazebo simulation environment.","sidebar":"tutorialSidebar"},"capstone/setup":{"id":"capstone/setup","title":"Capstone Project: Setup Guide","description":"This guide provides step-by-step instructions for setting up the environment to run the full capstone project. The project is designed to run in a ROS 2 and Gazebo environment on a Linux-based system (or WSL 2 on Windows).","sidebar":"tutorialSidebar"},"embodied-learning/diffusion-for-robotics":{"id":"embodied-learning/diffusion-for-robotics","title":"Diffusion Models for Robot Control","description":"Key insight: The same technology that powers DALL-E (image generation) can generate robot trajectories. Instead of generating pixels, we generate smooth, collision-free action sequences.","sidebar":"tutorialSidebar"},"embodied-learning/end-to-end-learning":{"id":"embodied-learning/end-to-end-learning","title":"End-to-End Learning: Architectures & Strategies","description":"Core principle: Map directly from raw observations (images, proprioception) to motor commands. No intermediate modules.","sidebar":"tutorialSidebar"},"embodied-learning/exercises":{"id":"embodied-learning/exercises","title":"Module 5: End-to-End Learning - Exercises","description":"Practical exercises applying behavioral cloning, diffusion models, and reinforcement learning to robot manipulation.","sidebar":"tutorialSidebar"},"embodied-learning/imitation-learning":{"id":"embodied-learning/imitation-learning","title":"Imitation Learning: Learning from Demonstrations","description":"Imitation learning is the simplest approach: watch demonstrations, predict actions, that's it. No reward signals. No trial-and-error. Just supervised learning.","sidebar":"tutorialSidebar"},"embodied-learning/intro":{"id":"embodied-learning/intro","title":"Module 5: End-to-End Learning & Diffusion Models","description":"Welcome to embodied learning - where robots learn directly from demonstration data instead of being explicitly programmed. This is the bridge between language understanding (Module 4) and real-world robotic execution.","sidebar":"tutorialSidebar"},"embodied-learning/learning-spectrum":{"id":"embodied-learning/learning-spectrum","title":"The Learning Spectrum: BC vs Diffusion vs RL","description":"Visualization of the complete learning paradigm spectrum for embodied AI.","sidebar":"tutorialSidebar"},"embodied-learning/reinforcement-learning":{"id":"embodied-learning/reinforcement-learning","title":"Reinforcement Learning for Embodied AI","description":"Core idea: Trial-and-error learning. Robot executes action, receives reward signal, learns to maximize cumulative reward.","sidebar":"tutorialSidebar"},"embodied-learning/training-pipeline":{"id":"embodied-learning/training-pipeline","title":"Complete Training Pipeline: From Data to Deployment","description":"End-to-end workflow for training robot manipulation policies.","sidebar":"tutorialSidebar"},"foundations/embodied-intelligence":{"id":"foundations/embodied-intelligence","title":"Embodied Intelligence","description":"What is Embodied Intelligence?","sidebar":"tutorialSidebar"},"foundations/exercises":{"id":"foundations/exercises","title":"Module 1: Exercises & Solutions","description":"These exercises reinforce the key concepts from Module 1: embodied intelligence, perception-action loops, and ROS 2 fundamentals.","sidebar":"tutorialSidebar"},"foundations/intro":{"id":"foundations/intro","title":"Module 1: Physical AI Foundations","description":"Overview","sidebar":"tutorialSidebar"},"foundations/ros2-intro":{"id":"foundations/ros2-intro","title":"ROS 2 Introduction","description":"What is ROS 2?","sidebar":"tutorialSidebar"},"glossary":{"id":"glossary","title":"Glossary","description":"This is a comprehensive glossary of terms used throughout the Physical AI & Humanoid Robotics textbook.","sidebar":"tutorialSidebar"},"index":{"id":"index","title":"Physical AI & Humanoid Robotics","description":"Welcome to the Physical AI & Humanoid Robotics textbook.","sidebar":"tutorialSidebar"},"perception/diagrams/domain-randomization":{"id":"perception/diagrams/domain-randomization","title":"Domain Randomization Visualization","description":"This document describes the domain randomization process and its effect on policy robustness.","sidebar":"tutorialSidebar"},"perception/diagrams/sim-to-real-gaps":{"id":"perception/diagrams/sim-to-real-gaps","title":"Sim-to-Real Transfer Gaps Visualization","description":"This document describes the diagram showing the four major gaps between simulation and reality.","sidebar":"tutorialSidebar"},"perception/exercises":{"id":"perception/exercises","title":"Module 3: Perception & Sim-to-Real Transfer Exercises","description":"These exercises guide you through applying sensor fusion, sim-to-real transfer, and domain randomization to real robotics challenges. Work through them in order, and compare your solutions with the provided solutions.","sidebar":"tutorialSidebar"},"perception/intro":{"id":"perception/intro","title":"Module 3: Perception & Sim-to-Real Transfer","description":"Learning Objectives","sidebar":"tutorialSidebar"},"perception/isaac-workflows":{"id":"perception/isaac-workflows","title":"NVIDIA Isaac: Photorealistic Simulation & Synthetic Data Generation","description":"You've learned how to bridge sim-to-real transfer using domain randomization. But some tasks—vision-intensive tasks, complex manipulation—need photorealistic rendering. This chapter covers NVIDIA Isaac, the industry-standard platform for photorealistic robot simulation.","sidebar":"tutorialSidebar"},"perception/sensor-fusion":{"id":"perception/sensor-fusion","title":"Sensor Fusion & Localization","description":"Humanoid robots live in complex, dynamic environments. A single sensor cannot see everything. Cameras see colors and shapes but fail in darkness. LiDAR sees depth but ignores texture. IMU detects motion but drifts over time. The solution: sensor fusion—combining multiple sensors to get a complete, accurate picture of the world.","sidebar":"tutorialSidebar"},"perception/setup-isaac":{"id":"perception/setup-isaac","title":"Setting Up NVIDIA Isaac Sim","description":"This guide covers installing NVIDIA Isaac Sim and configuring it for the Physical AI book exercises. Isaac Sim is the industry-standard photorealistic robot simulator used by companies like Boston Dynamics, Tesla, and major AI labs.","sidebar":"tutorialSidebar"},"perception/sim-to-real-transfer":{"id":"perception/sim-to-real-transfer","title":"Sim-to-Real Transfer: Bridging Simulation and Reality","description":"This is the core chapter of the Physical AI book. It explains why well-trained policies fail catastrophically on real robots and the systematic methods to fix them.","sidebar":"tutorialSidebar"},"references":{"id":"references","title":"References & Further Reading","description":"This page contains citations and resources referenced throughout the Physical AI & Humanoid Robotics textbook.","sidebar":"tutorialSidebar"},"scaling-systems/benchmarking-framework":{"id":"scaling-systems/benchmarking-framework","title":"Benchmarking at Scale: Evaluation Framework","description":"Measuring performance across 150+ tasks fairly and systematically.","sidebar":"tutorialSidebar"},"scaling-systems/cost-analysis":{"id":"scaling-systems/cost-analysis","title":"Cost Analysis: Economics of Robotic Automation","description":"When does automation become profitable?","sidebar":"tutorialSidebar"},"scaling-systems/distributed-training":{"id":"scaling-systems/distributed-training","title":"Distributed Training: Multi-GPU & Multi-Robot Systems","description":"Scaling training from single GPU to clusters and robot fleets.","sidebar":"tutorialSidebar"},"scaling-systems/exercises":{"id":"scaling-systems/exercises","title":"Exercises: Production Robotics & Scaling Systems","description":"Build and deploy production-quality robotic systems.","sidebar":"tutorialSidebar"},"scaling-systems/fleet-architecture":{"id":"scaling-systems/fleet-architecture","title":"Fleet Architecture: System Design at Scale","description":"Complete system design for 100+ robots learning together.","sidebar":"tutorialSidebar"},"scaling-systems/intro":{"id":"scaling-systems/intro","title":"Module 6: Scaling & Production Systems","description":"Welcome to production robotics - where you take a single robot learning model and scale it to hundreds of tasks, thousands of robots, and real-world deployment challenges.","sidebar":"tutorialSidebar"},"scaling-systems/multi-task-learning":{"id":"scaling-systems/multi-task-learning","title":"Multi-Task Learning: Training at Scale","description":"Training on 150+ manipulation tasks simultaneously.","sidebar":"tutorialSidebar"},"scaling-systems/real-robot-deployment":{"id":"scaling-systems/real-robot-deployment","title":"Real Robot Deployment: From Simulation to Production","description":"The hardest problem in robotics: making simulation work on real robots.","sidebar":"tutorialSidebar"},"scaling-systems/scaling-pipeline":{"id":"scaling-systems/scaling-pipeline","title":"Scaling Pipeline: End-to-End Production Workflow","description":"Complete pipeline from research prototype to production fleet.","sidebar":"tutorialSidebar"},"simulation/exercises":{"id":"simulation/exercises","title":"Module 2: Exercises & Solutions","description":"These exercises guide you through using Gazebo to simulate a humanoid robot. You'll load models, inspect sensor output, modify URDFs, and understand sim-to-real transfer challenges.","sidebar":"tutorialSidebar"},"simulation/gazebo-fundamentals":{"id":"simulation/gazebo-fundamentals","title":"Gazebo Fundamentals","description":"Gazebo is the industry-standard open-source 3D robotics simulator. It combines:","sidebar":"tutorialSidebar"},"simulation/intro":{"id":"simulation/intro","title":"Module 2: Digital Twins & Gazebo Simulation","description":"Learning Objectives","sidebar":"tutorialSidebar"},"simulation/setup-gazebo":{"id":"simulation/setup-gazebo","title":"Setting Up Gazebo for Humanoid Robotics","description":"This guide covers installing Gazebo 11+, configuring ROS 2 integration, and setting up the development environment for the Physical AI book exercises.","sidebar":"tutorialSidebar"},"simulation/urdf-humanoid":{"id":"simulation/urdf-humanoid","title":"URDF: Describing Humanoid Robots","description":"URDF (Unified Robot Description Format) is an XML language that describes robot structure, kinematics, inertias, and sensors. It's the standard format used across the ROS 2 ecosystem.","sidebar":"tutorialSidebar"},"troubleshooting":{"id":"troubleshooting","title":"Troubleshooting","description":"This guide covers common issues and solutions for the Physical AI & Humanoid Robotics textbook.","sidebar":"tutorialSidebar"},"vla-systems/action-grounding":{"id":"vla-systems/action-grounding","title":"Action Grounding in Robotics","description":"Introduction","sidebar":"tutorialSidebar"},"vla-systems/embodied-reasoning":{"id":"vla-systems/embodied-reasoning","title":"Embodied Reasoning in Robotics","description":"Introduction","sidebar":"tutorialSidebar"},"vla-systems/end-to-end-pipeline":{"id":"vla-systems/end-to-end-pipeline","title":"Vision-Language-Action Systems: End-to-End Pipeline","description":"Introduction","sidebar":"tutorialSidebar"},"vla-systems/exercises":{"id":"vla-systems/exercises","title":"Module 4 Exercises: Vision-Language-Action Systems","description":"These exercises are designed to test your understanding of how to design, build, and debug Vision-Language-Action (VLA) systems.","sidebar":"tutorialSidebar"},"vla-systems/intro":{"id":"vla-systems/intro","title":"Module 4: Vision-Language-Action Systems & Embodied AI","description":"Learning Objectives","sidebar":"tutorialSidebar"},"vla-systems/llm-planning":{"id":"vla-systems/llm-planning","title":"LLM-Based Task Planning for Robotics","description":"Large Language Models (LLMs) have shown remarkable capabilities in understanding and decomposing complex natural language commands into actionable steps. In robotics, this ability is transformative, allowing us to move from rigid, pre-programmed instructions to fluid, goal-oriented interactions.","sidebar":"tutorialSidebar"},"vla-systems/lora-adaptation":{"id":"vla-systems/lora-adaptation","title":"Fine-Tuning LLMs with LoRA Adaptation","description":"While few-shot prompting is powerful, sometimes you need to adapt a base Large Language Model (LLM) to a very specific domain or task. Fine-tuning is the process of updating the model's weights on a custom dataset. However, fine-tuning a full LLM with billions of parameters is computationally expensive and produces a massive new model file.","sidebar":"tutorialSidebar"},"vla-systems/prompting-strategies":{"id":"vla-systems/prompting-strategies","title":"Prompting Strategies for Robotics","description":"Introduction","sidebar":"tutorialSidebar"},"vla-systems/setup-llm":{"id":"vla-systems/setup-llm","title":"Setup Guide: Large Language Models for Robotics","description":"This guide provides instructions for setting up and running Large Language Models (LLMs) locally for use in your robotics projects. Running models locally is crucial for applications requiring low latency, data privacy, and high customization.","sidebar":"tutorialSidebar"},"vla-systems/vision-language-models":{"id":"vla-systems/vision-language-models","title":"Vision-Language Models for Robotics","description":"Introduction","sidebar":"tutorialSidebar"},"vla-systems/vla-architecture":{"id":"vla-systems/vla-architecture","title":"VLA System Architecture","description":"Introduction","sidebar":"tutorialSidebar"},"vla-systems/voice-to-action":{"id":"vla-systems/voice-to-action","title":"End-to-End Voice-to-Action Pipeline","description":"The ultimate goal of many VLA (Vision-Language-Action) systems is to enable natural, spoken-language interaction with a robot. This section details the architecture and implementation of a complete voice-to-action pipeline, integrating the concepts from the previous sections.","sidebar":"tutorialSidebar"}}}}