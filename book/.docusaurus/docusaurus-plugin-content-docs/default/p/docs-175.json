{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/docs/","label":"Physical AI & Humanoid Robotics","docId":"index","unlisted":false},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/docs/module-1-ros2/intro","label":"Module 1: The Robotic Nervous System","docId":"module-1-ros2/intro","unlisted":false},{"type":"link","href":"/docs/module-1-ros2/physical-ai-foundations","label":"Embodied Intelligence","docId":"module-1-ros2/physical-ai-foundations","unlisted":false},{"type":"link","href":"/docs/module-1-ros2/ros2-architecture","label":"ROS 2 Introduction","docId":"module-1-ros2/ros2-architecture","unlisted":false},{"type":"link","href":"/docs/module-1-ros2/nodes-topics-services","label":"Nodes, Topics, and Services","docId":"module-1-ros2/nodes-topics-services","unlisted":false},{"type":"link","href":"/docs/module-1-ros2/rclpy-python-agents","label":"Python Agents with rclpy","docId":"module-1-ros2/rclpy-python-agents","unlisted":false},{"type":"link","href":"/docs/module-1-ros2/launch-files","label":"Launch Files and Parameters","docId":"module-1-ros2/launch-files","unlisted":false},{"type":"link","href":"/docs/module-1-ros2/urdf-humanoids","label":"URDF: Describing Humanoid Robots","docId":"module-1-ros2/urdf-humanoids","unlisted":false},{"type":"link","href":"/docs/module-1-ros2/exercises","label":"Module 1: Exercises & Solutions","docId":"module-1-ros2/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/docs/module-2-digital-twin/intro","label":"Module 2: The Digital Twin","docId":"module-2-digital-twin/intro","unlisted":false},{"type":"link","href":"/docs/module-2-digital-twin/gazebo-fundamentals","label":"Gazebo Fundamentals","docId":"module-2-digital-twin/gazebo-fundamentals","unlisted":false},{"type":"link","href":"/docs/module-2-digital-twin/setup-gazebo","label":"Setting Up Gazebo for Humanoid Robotics","docId":"module-2-digital-twin/setup-gazebo","unlisted":false},{"type":"link","href":"/docs/module-2-digital-twin/unity-integration","label":"Unity Integration","docId":"module-2-digital-twin/unity-integration","unlisted":false},{"type":"link","href":"/docs/module-2-digital-twin/exercises","label":"Module 2: Exercises & Solutions","docId":"module-2-digital-twin/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","items":[{"type":"link","href":"/docs/module-3-isaac/intro","label":"Module 3: The AI-Robot Brain","docId":"module-3-isaac/intro","unlisted":false},{"type":"link","href":"/docs/module-3-isaac/isaac-sim","label":"NVIDIA Isaac: Photorealistic Simulation & Synthetic Data Generation","docId":"module-3-isaac/isaac-sim","unlisted":false},{"type":"link","href":"/docs/module-3-isaac/isaac-ros-vslam","label":"Isaac ROS VSLAM","docId":"module-3-isaac/isaac-ros-vslam","unlisted":false},{"type":"link","href":"/docs/module-3-isaac/nav2-bipedal","label":"Nav2 for Bipedal Robots","docId":"module-3-isaac/nav2-bipedal","unlisted":false},{"type":"link","href":"/docs/module-3-isaac/sensor-fusion","label":"Sensor Fusion & Localization","docId":"module-3-isaac/sensor-fusion","unlisted":false},{"type":"link","href":"/docs/module-3-isaac/sim-to-real-transfer","label":"Sim-to-Real Transfer: Bridging Simulation and Reality","docId":"module-3-isaac/sim-to-real-transfer","unlisted":false},{"type":"link","href":"/docs/module-3-isaac/humanoid-kinematics","label":"Humanoid Kinematics","docId":"module-3-isaac/humanoid-kinematics","unlisted":false},{"type":"link","href":"/docs/module-3-isaac/bipedal-locomotion","label":"Bipedal Locomotion","docId":"module-3-isaac/bipedal-locomotion","unlisted":false},{"type":"link","href":"/docs/module-3-isaac/setup-isaac","label":"Setting Up NVIDIA Isaac Sim","docId":"module-3-isaac/setup-isaac","unlisted":false},{"type":"link","href":"/docs/module-3-isaac/exercises","label":"Module 3: Perception & Sim-to-Real Transfer Exercises","docId":"module-3-isaac/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/docs/module-4-vla/intro","label":"Module 4: Vision-Language-Action Systems & Embodied AI","docId":"module-4-vla/intro","unlisted":false},{"type":"link","href":"/docs/module-4-vla/vision-language-models","label":"Vision-Language Models for Robotics","docId":"module-4-vla/vision-language-models","unlisted":false},{"type":"link","href":"/docs/module-4-vla/vla-architecture","label":"VLA System Architecture","docId":"module-4-vla/vla-architecture","unlisted":false},{"type":"link","href":"/docs/module-4-vla/whisper-integration","label":"Whisper Voice-to-Action","docId":"module-4-vla/whisper-integration","unlisted":false},{"type":"link","href":"/docs/module-4-vla/llm-planning","label":"LLM-Based Task Planning for Robotics","docId":"module-4-vla/llm-planning","unlisted":false},{"type":"link","href":"/docs/module-4-vla/conversational-robotics","label":"Conversational Robotics","docId":"module-4-vla/conversational-robotics","unlisted":false},{"type":"link","href":"/docs/module-4-vla/action-grounding","label":"Action Grounding in Robotics","docId":"module-4-vla/action-grounding","unlisted":false},{"type":"link","href":"/docs/module-4-vla/embodied-reasoning","label":"Embodied Reasoning in Robotics","docId":"module-4-vla/embodied-reasoning","unlisted":false},{"type":"link","href":"/docs/module-4-vla/end-to-end-pipeline","label":"Vision-Language-Action Systems: End-to-End Pipeline","docId":"module-4-vla/end-to-end-pipeline","unlisted":false},{"type":"link","href":"/docs/module-4-vla/capstone-project","label":"Capstone Project","docId":"module-4-vla/capstone-project","unlisted":false},{"type":"link","href":"/docs/module-4-vla/setup-llm","label":"Setup Guide: Large Language Models for Robotics","docId":"module-4-vla/setup-llm","unlisted":false},{"type":"link","href":"/docs/module-4-vla/exercises","label":"Module 4 Exercises: Vision-Language-Action Systems","docId":"module-4-vla/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Appendices","items":[{"type":"link","href":"/docs/glossary","label":"Glossary","docId":"glossary","unlisted":false},{"type":"link","href":"/docs/references","label":"References & Further Reading","docId":"references","unlisted":false},{"type":"link","href":"/docs/troubleshooting","label":"Troubleshooting","docId":"troubleshooting","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"glossary":{"id":"glossary","title":"Glossary","description":"This is a comprehensive glossary of terms used throughout the Physical AI & Humanoid Robotics textbook.","sidebar":"tutorialSidebar"},"index":{"id":"index","title":"Physical AI & Humanoid Robotics","description":"Welcome to the Physical AI & Humanoid Robotics textbook.","sidebar":"tutorialSidebar"},"module-1-ros2/exercises":{"id":"module-1-ros2/exercises","title":"Module 1: Exercises & Solutions","description":"These exercises reinforce the key concepts from Module 1: embodied intelligence, perception-action loops, and ROS 2 fundamentals.","sidebar":"tutorialSidebar"},"module-1-ros2/intro":{"id":"module-1-ros2/intro","title":"Module 1: The Robotic Nervous System","description":"Overview","sidebar":"tutorialSidebar"},"module-1-ros2/launch-files":{"id":"module-1-ros2/launch-files","title":"Launch Files and Parameters","description":"Overview","sidebar":"tutorialSidebar"},"module-1-ros2/nodes-topics-services":{"id":"module-1-ros2/nodes-topics-services","title":"Nodes, Topics, and Services","description":"Overview","sidebar":"tutorialSidebar"},"module-1-ros2/physical-ai-foundations":{"id":"module-1-ros2/physical-ai-foundations","title":"Embodied Intelligence","description":"What is Embodied Intelligence?","sidebar":"tutorialSidebar"},"module-1-ros2/rclpy-python-agents":{"id":"module-1-ros2/rclpy-python-agents","title":"Python Agents with rclpy","description":"Overview","sidebar":"tutorialSidebar"},"module-1-ros2/ros2-architecture":{"id":"module-1-ros2/ros2-architecture","title":"ROS 2 Introduction","description":"What is ROS 2?","sidebar":"tutorialSidebar"},"module-1-ros2/urdf-humanoids":{"id":"module-1-ros2/urdf-humanoids","title":"URDF: Describing Humanoid Robots","description":"URDF (Unified Robot Description Format) is an XML language that describes robot structure, kinematics, inertias, and sensors. It's the standard format used across the ROS 2 ecosystem.","sidebar":"tutorialSidebar"},"module-2-digital-twin/exercises":{"id":"module-2-digital-twin/exercises","title":"Module 2: Exercises & Solutions","description":"These exercises guide you through using Gazebo to simulate a humanoid robot. You'll load models, inspect sensor output, modify URDFs, and understand sim-to-real transfer challenges.","sidebar":"tutorialSidebar"},"module-2-digital-twin/gazebo-fundamentals":{"id":"module-2-digital-twin/gazebo-fundamentals","title":"Gazebo Fundamentals","description":"Gazebo is the industry-standard open-source 3D robotics simulator. It combines:","sidebar":"tutorialSidebar"},"module-2-digital-twin/intro":{"id":"module-2-digital-twin/intro","title":"Module 2: The Digital Twin","description":"Overview","sidebar":"tutorialSidebar"},"module-2-digital-twin/setup-gazebo":{"id":"module-2-digital-twin/setup-gazebo","title":"Setting Up Gazebo for Humanoid Robotics","description":"This guide covers installing Gazebo 11+, configuring ROS 2 integration, and setting up the development environment for the Physical AI book exercises.","sidebar":"tutorialSidebar"},"module-2-digital-twin/unity-integration":{"id":"module-2-digital-twin/unity-integration","title":"Unity Integration","description":"Overview","sidebar":"tutorialSidebar"},"module-3-isaac/bipedal-locomotion":{"id":"module-3-isaac/bipedal-locomotion","title":"Bipedal Locomotion","description":"Overview","sidebar":"tutorialSidebar"},"module-3-isaac/exercises":{"id":"module-3-isaac/exercises","title":"Module 3: Perception & Sim-to-Real Transfer Exercises","description":"These exercises guide you through applying sensor fusion, sim-to-real transfer, and domain randomization to real robotics challenges. Work through them in order, and compare your solutions with the provided solutions.","sidebar":"tutorialSidebar"},"module-3-isaac/humanoid-kinematics":{"id":"module-3-isaac/humanoid-kinematics","title":"Humanoid Kinematics","description":"Overview","sidebar":"tutorialSidebar"},"module-3-isaac/intro":{"id":"module-3-isaac/intro","title":"Module 3: The AI-Robot Brain","description":"Overview","sidebar":"tutorialSidebar"},"module-3-isaac/isaac-ros-vslam":{"id":"module-3-isaac/isaac-ros-vslam","title":"Isaac ROS VSLAM","description":"Overview","sidebar":"tutorialSidebar"},"module-3-isaac/isaac-sim":{"id":"module-3-isaac/isaac-sim","title":"NVIDIA Isaac: Photorealistic Simulation & Synthetic Data Generation","description":"You've learned how to bridge sim-to-real transfer using domain randomization. But some tasks—vision-intensive tasks, complex manipulation—need photorealistic rendering. This chapter covers NVIDIA Isaac, the industry-standard platform for photorealistic robot simulation.","sidebar":"tutorialSidebar"},"module-3-isaac/nav2-bipedal":{"id":"module-3-isaac/nav2-bipedal","title":"Nav2 for Bipedal Robots","description":"Overview","sidebar":"tutorialSidebar"},"module-3-isaac/sensor-fusion":{"id":"module-3-isaac/sensor-fusion","title":"Sensor Fusion & Localization","description":"Humanoid robots live in complex, dynamic environments. A single sensor cannot see everything. Cameras see colors and shapes but fail in darkness. LiDAR sees depth but ignores texture. IMU detects motion but drifts over time. The solution: sensor fusion—combining multiple sensors to get a complete, accurate picture of the world.","sidebar":"tutorialSidebar"},"module-3-isaac/setup-isaac":{"id":"module-3-isaac/setup-isaac","title":"Setting Up NVIDIA Isaac Sim","description":"This guide covers installing NVIDIA Isaac Sim and configuring it for the Physical AI book exercises. Isaac Sim is the industry-standard photorealistic robot simulator used by companies like Boston Dynamics, Tesla, and major AI labs.","sidebar":"tutorialSidebar"},"module-3-isaac/sim-to-real-transfer":{"id":"module-3-isaac/sim-to-real-transfer","title":"Sim-to-Real Transfer: Bridging Simulation and Reality","description":"This is the core chapter of the Physical AI book. It explains why well-trained policies fail catastrophically on real robots and the systematic methods to fix them.","sidebar":"tutorialSidebar"},"module-4-vla/action-grounding":{"id":"module-4-vla/action-grounding","title":"Action Grounding in Robotics","description":"Introduction","sidebar":"tutorialSidebar"},"module-4-vla/capstone-project":{"id":"module-4-vla/capstone-project","title":"Capstone Project","description":"Overview","sidebar":"tutorialSidebar"},"module-4-vla/conversational-robotics":{"id":"module-4-vla/conversational-robotics","title":"Conversational Robotics","description":"Overview","sidebar":"tutorialSidebar"},"module-4-vla/embodied-reasoning":{"id":"module-4-vla/embodied-reasoning","title":"Embodied Reasoning in Robotics","description":"Introduction","sidebar":"tutorialSidebar"},"module-4-vla/end-to-end-pipeline":{"id":"module-4-vla/end-to-end-pipeline","title":"Vision-Language-Action Systems: End-to-End Pipeline","description":"Introduction","sidebar":"tutorialSidebar"},"module-4-vla/exercises":{"id":"module-4-vla/exercises","title":"Module 4 Exercises: Vision-Language-Action Systems","description":"These exercises are designed to test your understanding of how to design, build, and debug Vision-Language-Action (VLA) systems.","sidebar":"tutorialSidebar"},"module-4-vla/intro":{"id":"module-4-vla/intro","title":"Module 4: Vision-Language-Action Systems & Embodied AI","description":"Learning Objectives","sidebar":"tutorialSidebar"},"module-4-vla/llm-planning":{"id":"module-4-vla/llm-planning","title":"LLM-Based Task Planning for Robotics","description":"Large Language Models (LLMs) have shown remarkable capabilities in understanding and decomposing complex natural language commands into actionable steps. In robotics, this ability is transformative, allowing us to move from rigid, pre-programmed instructions to fluid, goal-oriented interactions.","sidebar":"tutorialSidebar"},"module-4-vla/lora-adaptation":{"id":"module-4-vla/lora-adaptation","title":"Fine-Tuning LLMs with LoRA Adaptation","description":"While few-shot prompting is powerful, sometimes you need to adapt a base Large Language Model (LLM) to a very specific domain or task. Fine-tuning is the process of updating the model's weights on a custom dataset. However, fine-tuning a full LLM with billions of parameters is computationally expensive and produces a massive new model file."},"module-4-vla/prompting-strategies":{"id":"module-4-vla/prompting-strategies","title":"Prompting Strategies for Robotics","description":"Introduction"},"module-4-vla/setup-llm":{"id":"module-4-vla/setup-llm","title":"Setup Guide: Large Language Models for Robotics","description":"This guide provides instructions for setting up and running Large Language Models (LLMs) locally for use in your robotics projects. Running models locally is crucial for applications requiring low latency, data privacy, and high customization.","sidebar":"tutorialSidebar"},"module-4-vla/vision-language-models":{"id":"module-4-vla/vision-language-models","title":"Vision-Language Models for Robotics","description":"Introduction","sidebar":"tutorialSidebar"},"module-4-vla/vla-architecture":{"id":"module-4-vla/vla-architecture","title":"VLA System Architecture","description":"Introduction","sidebar":"tutorialSidebar"},"module-4-vla/voice-to-action":{"id":"module-4-vla/voice-to-action","title":"End-to-End Voice-to-Action Pipeline","description":"The ultimate goal of many VLA (Vision-Language-Action) systems is to enable natural, spoken-language interaction with a robot. This section details the architecture and implementation of a complete voice-to-action pipeline, integrating the concepts from the previous sections."},"module-4-vla/whisper-integration":{"id":"module-4-vla/whisper-integration","title":"Whisper Voice-to-Action","description":"Overview","sidebar":"tutorialSidebar"},"references":{"id":"references","title":"References & Further Reading","description":"This page contains citations and resources referenced throughout the Physical AI & Humanoid Robotics textbook.","sidebar":"tutorialSidebar"},"troubleshooting":{"id":"troubleshooting","title":"Troubleshooting","description":"This guide covers common issues and solutions for the Physical AI & Humanoid Robotics textbook.","sidebar":"tutorialSidebar"}}}}